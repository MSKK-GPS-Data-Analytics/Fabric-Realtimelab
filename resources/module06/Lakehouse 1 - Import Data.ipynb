{"cells":[{"cell_type":"markdown","id":"17df4242-a0ba-47a2-a7d9-4a3cd1fa0d8a","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Lakehouse 1: Import Data\n","\n","This notebook will download additional historic information and merge it into the raw_stock_data table. This additional data will help provide more interesting reports and data science exploration.\n","\n","Before starting, the raw_stock_data table should be receiving data regularly from the Eventstream."]},{"cell_type":"code","execution_count":null,"id":"de95bc6c-b751-4253-84f9-edb05e05496e","metadata":{},"outputs":[],"source":["import datetime\n","from datetime import timedelta\n","from delta.tables import *\n","from pyspark.sql.functions import *\n","\n","# change table name if not raw_stock_data\n","targetTableName = 'raw_stock_data'\n","daysToImport = 30\n","\n","minDateInRaw = datetime.datetime.utcnow() # will be recalculated in next cell"]},{"cell_type":"code","execution_count":null,"id":"442fff2f-7379-4b2f-9767-32946cd8cbbb","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# find earliest date in raw_stock_data table, or use today's date if none\n","\n","df_min_date = spark.sql(f\"SELECT coalesce(min(to_timestamp(timestamp)),current_date()) as minDate FROM {targetTableName}\")\n","minDateInRaw = df_min_date.first()[\"minDate\"]\n","print(f\"Min date in raw table: {minDateInRaw}\")\n","\n","# import data before current day\n","historicalEndDate = minDateInRaw.replace(hour=0, minute=0, second=0, microsecond=0)\n","historicalBeginDate = historicalEndDate + datetime.timedelta(days=-daysToImport)\n","\n","print(f'Historical import begin date: {historicalBeginDate}')\n","print(f'Historical import end date: {historicalEndDate}')"]},{"cell_type":"markdown","id":"cdb998e3-73cb-4f96-af09-f75cdedfedcd","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["## Download historical data\n","\n","The cells below will download and unzip historical data to the lakehouse unmanaged files."]},{"cell_type":"code","execution_count":null,"id":"160c1754","metadata":{},"outputs":[],"source":["class HistoryData:\n","    def __init__(self, file_uri, filename, year) -> None:\n","        self.file_uri = file_uri\n","        self.filename = filename\n","        self.year = year\n","\n","def getDownloadInfo(year):\n","    if year==2023:\n","        return HistoryData(\n","            'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2023.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=ledWmONUdRKvcpDumZHpLPqkrTLWu%2B9GrF0gMh5QK2c%3D',\n","            'stockhistory-2023.tgz',\n","            year)\n","    elif year==2024:\n","        return HistoryData(\n","            'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2024.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=TIFg2tvEww3rdTVNOKo5ef1xTx%2Bs0XAbdEARKGhOiX8%3D',\n","            'stockhistory-2024.tgz',\n","            year)\n","    elif year==2025:\n","        return HistoryData(\n","            'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2025.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=UB4QhOmsfwhPC0rE14wRJQxeiXXutHxm%2BOVnFA3xDFQ%3D',\n","            'stockhistory-2025.tgz',\n","            year)\n","    elif year==2026:\n","        return HistoryData(\n","            'https://fabricrealtimelab.blob.core.windows.net/public/AbboCost_Stock_History_v2/stockhistory-2026.tgz?sp=r&st=2024-01-01T17:00:00Z&se=2032-01-01T17:00:00Z&spr=https&sv=2022-11-02&sr=b&sig=l4tonO4SZfuCbHrheomO0WNkuYfyTTdfdNrcfu%2Fc7dU%3D',\n","            'stockhistory-2026.tgz',\n","            year)\n","    else:\n","        return None\n","\n"]},{"cell_type":"code","execution_count":null,"id":"71588929","metadata":{},"outputs":[],"source":["import os\n","import datetime\n","from datetime import timedelta\n","\n","LAKEHOUSE_FOLDER = \"/lakehouse/default\"\n","DATA_FOLDER = \"Files/stockhistory/raw\"\n","\n","TAR_FILE_PATH = f\"/{LAKEHOUSE_FOLDER}/{DATA_FOLDER}/tar/\"\n","CSV_FILE_PATH = f\"/{LAKEHOUSE_FOLDER}/{DATA_FOLDER}/csv/\"\n","\n","def downloadHistoryIfNotExists():\n","\n","    currYear = datetime.datetime.utcnow().year\n","\n","    if not os.path.exists(LAKEHOUSE_FOLDER):\n","        # add a lakehouse if the notebook has no default lakehouse\n","        # a new notebook will not link to any lakehouse by default\n","        raise FileNotFoundError(\n","            \"Lakehouse not found, please add a lakehouse for the notebook.\"\n","        )\n","    else:\n","        for year in range(currYear, currYear-2, -1):\n","            fileInfo = getDownloadInfo(year)\n","\n","            if (fileInfo is None):\n","                print(f'No file exists for {year}')\n","                continue\n","\n","            # verify if files are already in the lakehouse, and if not, download and unzip\n","            if not os.path.exists(f\"{TAR_FILE_PATH}{fileInfo.filename}\"):\n","                print(f'Downloading {fileInfo.filename}')\n","                os.makedirs(TAR_FILE_PATH, exist_ok=True)\n","                os.system(f\"wget '{fileInfo.file_uri}' -O {TAR_FILE_PATH}{fileInfo.filename}\")\n","\n","                #todo: better file checking\n","                os.makedirs(CSV_FILE_PATH, exist_ok=True)\n","                print(f'Extracting {fileInfo.filename}')\n","                os.system(f\"tar -zxvf {TAR_FILE_PATH}{fileInfo.filename} -C {CSV_FILE_PATH}\")\n","            else:\n","                print(f'File already exists: {fileInfo.filename}')\n","\n","downloadHistoryIfNotExists()"]},{"cell_type":"code","execution_count":null,"id":"56efbae5-d607-450a-8cc3-a96d57e7504d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# read the CSV files, {year}/{month}/{day}.csv\n","\n","df_stocks = (\n","    spark.read.format(\"csv\")\n","    .option(\"header\", \"true\")\n","    .load(f\"{DATA_FOLDER}/csv/*/*/*.csv\")\n",")\n","\n","df_stocks.tail(8)"]},{"cell_type":"code","execution_count":null,"id":"dfed41be-e310-4089-8600-a737aa17a799","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# filter stocks to between min/max dates\n","\n","df_stocks = df_stocks.select(\"*\").where( \\\n","    f'timestamp >= \"{str(historicalBeginDate)}\" and timestamp < \"{str(historicalEndDate)}\"').sort(\"timestamp\")\n","\n","df_stocks.tail(8)"]},{"cell_type":"code","execution_count":null,"id":"9c818805","metadata":{},"outputs":[],"source":["# if target column is a timestamp, cast it to timestamp\n","# default is string, but when doing merges it's important data types match\n","# to avoid potential locking issues\n","\n","# from pyspark.sql import types as T\n","# df_stocks = df_stocks.withColumn('timestamp', df_stocks['timestamp'].cast(T.TimestampType()))"]},{"cell_type":"code","execution_count":null,"id":"b370e070-f9c2-47e9-a7e3-d8be4090eda0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# to insert the new data, we'll merge the dataframe with the raw table\n","\n","from delta.tables import *\n","\n","def importIntoRaw(df):\n","\n","  raw_table = DeltaTable.forName(spark, targetTableName)\n","\n","  raw_table.alias('raw') \\\n","    .merge(\n","      df.alias('history'),\n","      'raw.timestamp = history.timestamp and raw.symbol = history.symbol'\n","    ) \\\n","    .whenNotMatchedInsert(values =\n","      {\n","          \"symbol\": \"history.symbol\"\n","          ,\"price\": \"history.price\"\n","          ,\"timestamp\": \"history.timestamp\"\n","      }\n","    ) \\\n","    .execute()\n","\n","importIntoRaw(df_stocks)"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{},"enableDebugMode":false}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{}},"widgets":{}},"nbformat":4,"nbformat_minor":5}
